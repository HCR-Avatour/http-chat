name: llm
services:
    local_llm:
        container_name: llm
        build: .
        ports:
            - 9542:80
        volumes:
            - /tmp/argus_socket:/tmp/argus_socket
            - /etc/enctune.conf:/etc/enctune.conf
            - /etc/nv_tegra_release:/etc/nv_tegra_release
            - /var/run/dbus:/var/run/dbus
            - /var/run/avahi-daemon/socket:/var/run/avahi-daemon/socket
            - /var/run/docker.sock:/var/run/docker.sock
            - /ssd/jetson-containers/data:/data
        # command: python3 -m local_llm.http_chat --api=mlc --model princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT #--max-new-tokens=100
        # command: python3 -m local_llm.http_chat --api=mlc --model=meta-llama/Llama-2-7b-chat-hf --chat-template=llama-2 --repetition-penalty=1.5 --temperature=1.0 --max-new-tokens=50
        command: python3 -m local_llm.http_chat --api=mlc --model=meta-llama/Llama-2-13b-chat-hf --chat-template=llama-2 --repetition-penalty=1.0 --temperature=1.0 --max-new-tokens=50
        # command: python3 -m local_llm.agents.video_query --api=mlc \
        #   --model Efficient-Large-Model/VILA-2.7b \
        #   --max-context-len 768 \
        #   --max-new-tokens 32 \
        #   --video-input stream.avatour.duckdns.org/theta \
        #   --video-output /mount/output.mp4 \
        #   --prompt "What does the weather look like?"